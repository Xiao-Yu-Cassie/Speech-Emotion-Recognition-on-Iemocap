{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "707proj.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "IG7ZEkY0go6j",
        "ywWeAsaF1eVe",
        "yXpYEjzJIT6O",
        "inBeJ8tyz6mM",
        "j2KQxarN1ffe",
        "vk8I4lkJ18FR",
        "gY7Upqnt2XuZ"
      ],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e07be73e26f842e8875fa5aa41221e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7607def5cfc44047a03a2f7c33f23639",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8a97bcb4894149949fa26426a673f597",
              "IPY_MODEL_37b9f7f3cc204594bd0feb561ca31393"
            ]
          }
        },
        "7607def5cfc44047a03a2f7c33f23639": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8a97bcb4894149949fa26426a673f597": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a2660f8b4f3a423189f28e8b4a2cb852",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_30f712435bff403985efb091ef6c383a"
          }
        },
        "37b9f7f3cc204594bd0feb561ca31393": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7d58e5d9769a4ecc842dfb491e4f0ca7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 344kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6d05673f2f80450ba235e7af1385cf8c"
          }
        },
        "a2660f8b4f3a423189f28e8b4a2cb852": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "30f712435bff403985efb091ef6c383a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7d58e5d9769a4ecc842dfb491e4f0ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6d05673f2f80450ba235e7af1385cf8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "136dba1ffeee435ab9473447f9dee164": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7234b4e432ba4c8ca555af1de3251c90",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_08214e285cb74da39b80c44f310401e7",
              "IPY_MODEL_36a2efe0d2694db9bbeeb2713b5593e2"
            ]
          }
        },
        "7234b4e432ba4c8ca555af1de3251c90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "08214e285cb74da39b80c44f310401e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f86218eec90543f9a6fcb08d97094d38",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 28,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 28,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_99cd69fabf86415b9add84fcecd47e24"
          }
        },
        "36a2efe0d2694db9bbeeb2713b5593e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_28368506dd3c479e9f9d7a5cca04537c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 28.0/28.0 [00:00&lt;00:00, 94.9B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bbe51187e2944f66a1dc80a49a67f4b4"
          }
        },
        "f86218eec90543f9a6fcb08d97094d38": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "99cd69fabf86415b9add84fcecd47e24": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "28368506dd3c479e9f9d7a5cca04537c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bbe51187e2944f66a1dc80a49a67f4b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "776f4d1c3a044e57a0cbf00762cd1ea0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_92ce866e38824087bab4ccb99f3a5c62",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_ce0f6aeedcee422487a3aae10175f828",
              "IPY_MODEL_7864293d1d744fd9b3a77f7adbfad2cd"
            ]
          }
        },
        "92ce866e38824087bab4ccb99f3a5c62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ce0f6aeedcee422487a3aae10175f828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ed4db6c1ee05426daec637c37b27d703",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 466062,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 466062,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f9f865c718048c2b672263f2cb48746"
          }
        },
        "7864293d1d744fd9b3a77f7adbfad2cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c0373bcb3ce6449a8e626b5919c084d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 466k/466k [00:00&lt;00:00, 3.86MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7357b20deceb4120bd9c939503405725"
          }
        },
        "ed4db6c1ee05426daec637c37b27d703": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f9f865c718048c2b672263f2cb48746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0373bcb3ce6449a8e626b5919c084d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7357b20deceb4120bd9c939503405725": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "233684a15fb14247889736f8d7b9ac2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_db501dee73394b5e89bb9294bd653098",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_44019b6d33a44cd684a9f90b1dd96135",
              "IPY_MODEL_b68af35427784eff97010fa54bca3eed"
            ]
          }
        },
        "db501dee73394b5e89bb9294bd653098": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "44019b6d33a44cd684a9f90b1dd96135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d3b1f75d1093478bbe2ed60bafe9d395",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 570,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 570,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e16e816b3a0045a291ff048096258855"
          }
        },
        "b68af35427784eff97010fa54bca3eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7f0c3107f36e4602856bc6a4292311d9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 570/570 [00:00&lt;00:00, 5.80kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd24ab0e163846378b4f89afdae021ab"
          }
        },
        "d3b1f75d1093478bbe2ed60bafe9d395": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e16e816b3a0045a291ff048096258855": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7f0c3107f36e4602856bc6a4292311d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd24ab0e163846378b4f89afdae021ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0a99013f55564a15890f9a5354ec5b21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_384457eafea24fea90cf8b4760d8f878",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_170cf4018a0c4005b482132b27d92994",
              "IPY_MODEL_b63936009dc74a19ab2979f4830a0d96"
            ]
          }
        },
        "384457eafea24fea90cf8b4760d8f878": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "170cf4018a0c4005b482132b27d92994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_acd4c3351a334b7a9a37106be30670b4",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_860630513ae346c8b8e0034beaf0f8ed"
          }
        },
        "b63936009dc74a19ab2979f4830a0d96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1b805630883b48a6b1c6f4c649a3b575",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:08&lt;00:00, 50.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a1891e7855644b0eaf9082860144e360"
          }
        },
        "acd4c3351a334b7a9a37106be30670b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "860630513ae346c8b8e0034beaf0f8ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "1b805630883b48a6b1c6f4c649a3b575": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a1891e7855644b0eaf9082860144e360": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xiao-Yu-Cassie/Speech-Emotion-Recognition-on-Iemocap/blob/main/707proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iQYzi5qoeiJQ"
      },
      "source": [
        "#Check Hardware Info"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kd083PX3etE9",
        "outputId": "8ce3a8e2-c82f-45a8-9b95-2fcd4579c6dc"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri May  7 18:11:00 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FVpr9NtfA5L",
        "outputId": "69e3b788-a042-4c02-f2b9-b7f38102d2e9"
      },
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Your runtime has 27.4 gigabytes of available RAM\n",
            "\n",
            "You are using a high-RAM runtime!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rI0gz_9yzJXU",
        "outputId": "e2c70ad8-181d-4a6f-8d49-6b5f1716f067"
      },
      "source": [
        "!pip install -U torchtext==0.8.0\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting torchtext==0.8.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/8a/e09b9b82d4dd676f17aa681003a7533765346744391966dec0d5dba03ee4/torchtext-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (6.9MB)\n",
            "\u001b[K     |████████████████████████████████| 7.0MB 5.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (1.8.1+cu101)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.8.0) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.8.0) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.8.0) (3.0.4)\n",
            "Installing collected packages: torchtext\n",
            "  Found existing installation: torchtext 0.9.1\n",
            "    Uninstalling torchtext-0.9.1:\n",
            "      Successfully uninstalled torchtext-0.9.1\n",
            "Successfully installed torchtext-0.8.0\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 6.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 21.7MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 35.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG7ZEkY0go6j"
      },
      "source": [
        "#Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUoRL94OgteJ"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from IPython.display import Image\n",
        "from IPython.core.display import Image, display\n",
        "from torchvision.utils import save_image\n",
        "import time\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import scipy\n",
        "import pickle\n",
        "#from bert_embedding import BertEmbedding\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5sCcQtLjgv0l",
        "outputId": "64bda64b-e0d8-4dff-f15c-f12e7bbef287"
      },
      "source": [
        "cuda = torch.cuda.is_available()\n",
        "device = torch.device('cuda' if cuda else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Mlg7gWFKB64"
      },
      "source": [
        "#Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FREEkRmO1Krh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d6422d2-d4d4-4298-ed00-825d7bdfe642"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1QqePzpCnlw"
      },
      "source": [
        "#Single-Utterance-based LSTM with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WM6A2we6Cm9Z"
      },
      "source": [
        "import torch.nn as nn\n",
        "class LSTM_Classifer(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, class_num, dropout=0.5):\n",
        "    super().__init__()\n",
        "    self.class_num = class_num\n",
        "    self.dropout = dropout\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim)\n",
        "    self.output_dim = output_dim\n",
        "   \n",
        "    self.fc1 = nn.Linear(self.hidden_dim, output_dim)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.output_layer = nn.Linear(output_dim, class_num)\n",
        "    self.dropoutLayer = nn.Dropout(self.dropout)\n",
        "\n",
        "  def attention(self, lstm_out, inp):\n",
        "    permuted_output = lstm_out.permute(1, 0, 2) #(batch, seqlen, hidden)\n",
        "    inp = torch.squeeze(inp, 0) #size (batch, hidden_size)\n",
        "    inp_tmp = torch.unsqueeze(inp, 2)\n",
        "    attn_weights = torch.bmm(permuted_output, inp_tmp) # (batch, seqlen, 1)\n",
        "    attn_weights = torch.squeeze(attn_weights, 2) # (batch, seqlen)\n",
        "    softmax_weights = F.softmax(attn_weights, 1) # (batch, seqlen)\n",
        "    softmax_weights = torch.unsqueeze(softmax_weights, 2) #(batch, seqlen, 1)\n",
        "    new_hidden_state = torch.bmm(permuted_output.transpose(1, 2), softmax_weights)\n",
        "    new_hidden_state = torch.squeeze(new_hidden_state, 2) #(batch, hidden)\n",
        "    return new_hidden_state\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[1]\n",
        "    lstm_out, (hidden, c) = self.lstm(x)\n",
        "    attn_output = self.attention(lstm_out, hidden)\n",
        "    output = self.fc1(attn_output)\n",
        "    output = self.act1(output)\n",
        "    output = self.output_layer(output)\n",
        "    return output\n",
        "\n",
        "  def encode_feature(self, x):\n",
        "    batch_size = x.shape[1]\n",
        "    lstm_out, (hidden, c) = self.lstm(x)\n",
        "    attn_output = self.attention(lstm_out, hidden)\n",
        "    return attn_output\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSsyFb2st91T"
      },
      "source": [
        "#torchtext Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "id": "PkZAov9AaBav",
        "outputId": "589ab7d0-c6db-482b-a4e3-4721623a979c"
      },
      "source": [
        "pip install numpy --upgrade"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/73/ef/8967d406f3f85018ceb5efab50431e901683188f1741ceb053efcab26c87/numpy-1.20.2-cp37-cp37m-manylinux2010_x86_64.whl (15.3MB)\n",
            "\u001b[K     |████████████████████████████████| 15.3MB 350kB/s \n",
            "\u001b[31mERROR: tensorflow 2.4.1 has requirement numpy~=1.19.2, but you'll have numpy 1.20.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "Successfully installed numpy-1.20.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sNqzV7PvLs6"
      },
      "source": [
        "import numpy\n",
        "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, BertModel\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7HUhK-4fwciZ",
        "outputId": "42bca09b-06b3-4c18-e76a-e501f40814dc"
      },
      "source": [
        "torch.__version__"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.8.1+cu101'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFk2r5d3vwtD"
      },
      "source": [
        "from torchtext.data import Field, TabularDataset, BucketIterator, Iterator, LabelField"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tm7f3IH4wHi2"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSyVzYQlwOLe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166,
          "referenced_widgets": [
            "e07be73e26f842e8875fa5aa41221e6a",
            "7607def5cfc44047a03a2f7c33f23639",
            "8a97bcb4894149949fa26426a673f597",
            "37b9f7f3cc204594bd0feb561ca31393",
            "a2660f8b4f3a423189f28e8b4a2cb852",
            "30f712435bff403985efb091ef6c383a",
            "7d58e5d9769a4ecc842dfb491e4f0ca7",
            "6d05673f2f80450ba235e7af1385cf8c",
            "136dba1ffeee435ab9473447f9dee164",
            "7234b4e432ba4c8ca555af1de3251c90",
            "08214e285cb74da39b80c44f310401e7",
            "36a2efe0d2694db9bbeeb2713b5593e2",
            "f86218eec90543f9a6fcb08d97094d38",
            "99cd69fabf86415b9add84fcecd47e24",
            "28368506dd3c479e9f9d7a5cca04537c",
            "bbe51187e2944f66a1dc80a49a67f4b4",
            "776f4d1c3a044e57a0cbf00762cd1ea0",
            "92ce866e38824087bab4ccb99f3a5c62",
            "ce0f6aeedcee422487a3aae10175f828",
            "7864293d1d744fd9b3a77f7adbfad2cd",
            "ed4db6c1ee05426daec637c37b27d703",
            "3f9f865c718048c2b672263f2cb48746",
            "c0373bcb3ce6449a8e626b5919c084d0",
            "7357b20deceb4120bd9c939503405725"
          ]
        },
        "outputId": "22a1a489-33e9-4976-c99f-84d570657da7"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e07be73e26f842e8875fa5aa41221e6a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "136dba1ffeee435ab9473447f9dee164",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "776f4d1c3a044e57a0cbf00762cd1ea0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfI_qhKiyvq8"
      },
      "source": [
        "init_token = tokenizer.cls_token\n",
        "eos_token = tokenizer.sep_token\n",
        "pad_token = tokenizer.pad_token\n",
        "unk_token = tokenizer.unk_token"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjeaPnm5zViY",
        "outputId": "119e7afd-531c-43de-d115-48005d47777e"
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']\n",
        "print(max_input_length)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdH4CZ7Gz3wZ"
      },
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGxt7iJvvdTC"
      },
      "source": [
        "init_token_idx = tokenizer.cls_token_id\n",
        "eos_token_idx = tokenizer.sep_token_id\n",
        "pad_token_idx = tokenizer.pad_token_id\n",
        "unk_token_idx = tokenizer.unk_token_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwpwDOP-vgPR"
      },
      "source": [
        "max_input_length = tokenizer.max_model_input_sizes['bert-base-uncased']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lw2XKllEvj4a"
      },
      "source": [
        "def tokenize_and_cut(sentence):\n",
        "    tokens = tokenizer.tokenize(sentence) \n",
        "    tokens = tokens[:max_input_length-2]\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrRM0kxOvmu5",
        "outputId": "b0194a09-8237-4611-e847-9c46a5090dc2"
      },
      "source": [
        "TEXT = Field(batch_first = True,\n",
        "                  use_vocab = False,\n",
        "                  tokenize = tokenize_and_cut,\n",
        "                  preprocessing = tokenizer.convert_tokens_to_ids,\n",
        "                  init_token = init_token_idx,\n",
        "                  eos_token = eos_token_idx,\n",
        "                  pad_token = pad_token_idx,\n",
        "                  unk_token = unk_token_idx)\n",
        "\n",
        "LABEL = LabelField(dtype = torch.float)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: Field class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/field.py:150: UserWarning: LabelField class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.\n",
            "  warnings.warn('{} class will be retired soon and moved to torchtext.legacy. Please see the most recent release notes for further information.'.format(self.__class__.__name__), UserWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdWf-Iz1zajW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116,
          "referenced_widgets": [
            "233684a15fb14247889736f8d7b9ac2f",
            "db501dee73394b5e89bb9294bd653098",
            "44019b6d33a44cd684a9f90b1dd96135",
            "b68af35427784eff97010fa54bca3eed",
            "d3b1f75d1093478bbe2ed60bafe9d395",
            "e16e816b3a0045a291ff048096258855",
            "7f0c3107f36e4602856bc6a4292311d9",
            "cd24ab0e163846378b4f89afdae021ab",
            "0a99013f55564a15890f9a5354ec5b21",
            "384457eafea24fea90cf8b4760d8f878",
            "170cf4018a0c4005b482132b27d92994",
            "b63936009dc74a19ab2979f4830a0d96",
            "acd4c3351a334b7a9a37106be30670b4",
            "860630513ae346c8b8e0034beaf0f8ed",
            "1b805630883b48a6b1c6f4c649a3b575",
            "a1891e7855644b0eaf9082860144e360"
          ]
        },
        "outputId": "f71a5bfa-70d5-4456-b606-3967931ffab5"
      },
      "source": [
        "bert_model = BertModel.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "233684a15fb14247889736f8d7b9ac2f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=570.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a99013f55564a15890f9a5354ec5b21",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaMRPTnSeHAk"
      },
      "source": [
        "#Text and Audio Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Lj2pdlmf0UH",
        "outputId": "d14cafe9-c3c2-442b-b197-e42bf6dfd19f"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "stemmer= PorterStemmer()\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "text = \"Nick likes to play football, however he is not too fond of tennis.\"\n",
        "text_tokens = word_tokenize(text)\n",
        "tokens_without_sw = [word for word in text_tokens if not word in stopwords.words()]\n",
        "print(tokens_without_sw)\n",
        "\n",
        "# From Poria's official code https://github.com/soujanyaporia/multimodal-sentiment-analysis/blob/master/dataset/iemocap/raw/IEMOCAP_features_raw.pkl.zip\n",
        "videoIDs, videoSpeakers, videoLabels, videoText, videoAudio, videoVisual, videoSentence, trainVid, testVid = pickle.load(open(\"gdrive/MyDrive/IEMOCAP_features_raw.pkl\", \"rb\"), encoding='latin1')\n",
        "\n",
        "#train_size: 120 videos.  test_size: 31 videos\n",
        "\n",
        "\n",
        "def generate_tokens(sentence):\n",
        "  sentence = sentence.lower()\n",
        "  sentence = sentence.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
        "  sentence = sentence.strip()\n",
        "  sentence_tokenize = word_tokenize(sentence)\n",
        "  sentence_no_stopwords = [word for word in sentence_tokenize if not word in stopwords.words()]\n",
        "  res = sentence_no_stopwords\n",
        "  res = [lemmatizer.lemmatize(word) for word in res]\n",
        "  res = ['[CLS]'] + res + ['[SEP]']\n",
        "  return res\n",
        "\n",
        "#padding + bertembeded\n",
        "def bert_embed(tokens, max_len):\n",
        "  padded_tokens = tokens + ['[PAD]' for _ in range(max_len-len(tokens))]\n",
        "  attn_mask = [ 1 if token != '[PAD]' else 0 for token in padded_tokens]\n",
        "  seg_ids=[0 for _ in range(len(padded_tokens))]\n",
        "  sent_ids=tokenizer.convert_tokens_to_ids(padded_tokens)\n",
        "  token_ids = torch.tensor(sent_ids).unsqueeze(0) \n",
        "  attn_mask = torch.tensor(attn_mask).unsqueeze(0) \n",
        "  seg_ids  = torch.tensor(seg_ids).unsqueeze(0)\n",
        "  output = bert_model(token_ids, attention_mask = attn_mask,token_type_ids = seg_ids)\n",
        "\n",
        "  attn_mask = attn_mask.float()\n",
        "\n",
        "\n",
        "  return (output.last_hidden_state)[0] , attn_mask\n",
        "\n",
        "\n",
        "def get_iemocap_raw():\n",
        "    train_text = []\n",
        "    train_seq_len = []\n",
        "    train_label = []\n",
        "\n",
        "    test_text = []\n",
        "    test_seq_len = []\n",
        "    test_label = []\n",
        "\n",
        "    for vid in trainVid:\n",
        "        for i in range(len(videoSentence[vid])):\n",
        "          cur_sentence = videoSentence[vid][i]\n",
        "          cur_label = videoLabels[vid][i]\n",
        "          sentence_tokens = generate_tokens(cur_sentence)\n",
        "          train_text.append(sentence_tokens)\n",
        "          train_label.append(cur_label)\n",
        "    \n",
        "    for vid in testVid:\n",
        "        for i in range(len(videoSentence[vid])):\n",
        "          cur_sentence = videoSentence[vid][i]\n",
        "          cur_label = videoLabels[vid][i]\n",
        "          sentence_tokens = generate_tokens(cur_sentence)\n",
        "          test_text.append(sentence_tokens)\n",
        "          test_label.append(cur_label)\n",
        "    \n",
        "    return train_text, train_label, test_text, test_label\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "['Nick', 'likes', 'play', 'football', ',', 'however', 'fond', 'tennis', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iL4yXzHPsI8I"
      },
      "source": [
        "train_text, train_label, test_text, test_label = get_iemocap_raw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K7SPPW4A0VwO"
      },
      "source": [
        "#Audio loading\n",
        "Audio_file =  pickle.load(open(\"gdrive/MyDrive/audio.pickle\", \"rb\"), encoding='latin1')\n",
        "def load_audio_raw():\n",
        "  train_wav = []\n",
        "  train_sample_rate = []\n",
        "  train_activation = []\n",
        "  train_valence = []\n",
        "  train_dominance = []\n",
        "  train_label = []\n",
        "\n",
        "\n",
        "  test_wav = []\n",
        "  test_sample_rate = []\n",
        "  test_activation = []\n",
        "  test_valence = []\n",
        "  test_dominance = []\n",
        "  test_label = []\n",
        "\n",
        "  for vid in trainVid:\n",
        "    for key in Audio_file[vid].keys():\n",
        "      cur_wav = Audio_file[vid][key]['waveform']\n",
        "      cur_sample_rate = Audio_file[vid][key]['sample_rate']\n",
        "      cur_emotion = Audio_file[vid][key]['emotion']\n",
        "      cur_activation = Audio_file[vid][key]['activation']\n",
        "      cur_valence = Audio_file[vid][key]['valence']\n",
        "      cur_dominance = Audio_file[vid][key]['dominance']\n",
        "\n",
        "      train_wav.append(cur_wav)\n",
        "      train_sample_rate.append(cur_sample_rate)\n",
        "      train_activation.append(cur_activation)\n",
        "      train_valence.append(cur_valence)\n",
        "      train_dominance.append(cur_dominance)\n",
        "\n",
        "      train_label.append(cur_emotion)\n",
        "    \n",
        "  for vid in testVid:\n",
        "    for key in Audio_file[vid].keys():\n",
        "      cur_wav = Audio_file[vid][key]['waveform']\n",
        "      cur_sample_rate = Audio_file[vid][key]['sample_rate']\n",
        "      cur_emotion = Audio_file[vid][key]['emotion']\n",
        "      cur_activation = Audio_file[vid][key]['activation']\n",
        "      cur_valence = Audio_file[vid][key]['valence']\n",
        "      cur_dominance = Audio_file[vid][key]['dominance']\n",
        "\n",
        "      test_wav.append(cur_wav)\n",
        "      test_sample_rate.append(cur_sample_rate)\n",
        "      test_activation.append(cur_activation)\n",
        "      test_valence.append(cur_valence)\n",
        "      test_dominance.append(cur_dominance)\n",
        "\n",
        "      test_label.append(cur_emotion)\n",
        "  \n",
        "  train_info = {\n",
        "      'wav': train_wav,\n",
        "      'sample_rate': train_sample_rate,\n",
        "      'activation': train_activation,\n",
        "      'dominance': train_dominance,\n",
        "      'valence': train_valence,\n",
        "      'label': train_label\n",
        "  }\n",
        "\n",
        "  test_info = {\n",
        "      'wav': test_wav,\n",
        "      'sample_rate': test_sample_rate,\n",
        "      'activation': test_activation,\n",
        "      'dominance': test_dominance,\n",
        "      'valence': test_valence,\n",
        "      'label': test_label\n",
        "  }\n",
        "\n",
        "\n",
        "    \n",
        "  return train_info, test_info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9g47my8lYXyy"
      },
      "source": [
        "train_audio_info, test_audio_info = load_audio_raw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsII_ABjt4I7"
      },
      "source": [
        "#Raw Text Dataset Class "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ldbCKsMPt6-2"
      },
      "source": [
        "class TextRaw_dataset(Dataset):\n",
        "    def __init__(self, tokens, labels, max_len):\n",
        "        self.data = tokens\n",
        "        self.labels = labels\n",
        "        self.max_len = max_len\n",
        "    def __getitem__(self, idx):\n",
        "        text_tokens = self.data[idx]\n",
        "        label = [float(self.labels[idx])]\n",
        "        embeded, mask = bert_embed(text_tokens, self.max_len)\n",
        "        mask = torch.Tensor(mask).float()\n",
        "        label = torch.Tensor(label).float()\n",
        "\n",
        "\n",
        "        embeded_no_grad = embeded.detach()\n",
        "        mask.requires_grad = False\n",
        "        label.requires_grad = False\n",
        "        \n",
        "        return (embeded_no_grad, mask, label)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-Uq5pOwrw-W"
      },
      "source": [
        "# Train-text modality\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWfKELRJr5CS"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "%matplotlib inline\n",
        "\n",
        "class TrainRaw():\n",
        "  def __init__(self):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(self.device)\n",
        "    self.max_len = 49\n",
        "    self.batch_size = 35\n",
        "    self.plot_every = 83 # plot the avg loss of 8 batches\n",
        "\n",
        "    #get iemocap text data\n",
        "    self.train_text, self.train_label, self.test_text, self.test_label = train_text, train_label, test_text, test_label # get_iemocap_raw()\n",
        "    self.trainRaw = TextRaw_dataset(self.train_text, self.train_label, self.max_len)\n",
        "    self.testRaw = TextRaw_dataset(self.test_text, self.test_label, self.max_len)\n",
        "\n",
        "    self.dataloader1 = DataLoader(self.trainRaw, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "    self.dataloader2 = DataLoader(self.testRaw, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "    \n",
        "    self.net = LSTM_Classifer(input_dim=768, hidden_dim=100, output_dim=100, class_num=6).to(self.device)\n",
        "\n",
        "    self.plot_train_losses = []\n",
        "    self.plot_test_losses = []\n",
        "    self.test_acc_l = []\n",
        "    self.train_acc_l = []\n",
        "\n",
        "    self.best_acc = 0\n",
        "    self.train()\n",
        "\n",
        "   \n",
        "\n",
        "  def train(self):\n",
        "    EPOCHS = 20\n",
        "    self.optimizer = optim.Adam(self.net.parameters(), lr=0.0001)\n",
        "    self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.75, patience=2, verbose=True)\n",
        "    self.loss_function = nn.CrossEntropyLoss()\n",
        "    self.test_loss()\n",
        "    for epoch in range(EPOCHS):\n",
        "      batch_index = 1\n",
        "      loss_sum = 0\n",
        "      correct_cnt = 0\n",
        "      for  X, mask, label in tqdm(self.dataloader1):\n",
        "        self.optimizer.zero_grad()\n",
        "        batch_index += 1\n",
        "        X = X.permute([1, 0, 2])\n",
        "        X = X.to(self.device)\n",
        "        label = label.long().to(self.device)\n",
        "        label = label.reshape(self.batch_size)\n",
        "        output = self.net(X) \n",
        "        loss = self.loss_function(output, label)     \n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        # save for train acc\n",
        "        pred = torch.max(output, axis=1).indices      \n",
        "        eq = torch.eq(label, pred)     \n",
        "        correct_cnt += torch.sum(eq)\n",
        "\n",
        "        # calculate loss and accuracy\n",
        "        if (batch_index % self.plot_every == 0):\n",
        "          avg_loss = loss_sum / (self.plot_every * self.batch_size)\n",
        "          #print(avg_loss)\n",
        "          self.plot_train_losses.append(avg_loss)\n",
        "          loss_sum = 0\n",
        "\n",
        "          test_loss, test_acc = self.test_loss()\n",
        "          self.plot_test_losses.append(test_loss)\n",
        "          self.test_acc_l.append(test_acc)\n",
        "\n",
        "          train_acc = float(correct_cnt / (self.plot_every * self.batch_size))\n",
        "          correct_cnt = 0\n",
        "          self.train_acc_l.append(train_acc)\n",
        "\n",
        "          print(f\"Epoch: {epoch}. Train Loss: {avg_loss}. Train Acc: {train_acc}. Test Loss: {test_loss}. Test Acc: {test_acc}\")\n",
        "          self.plot(self.plot_train_losses, self.plot_test_losses, \"train_loss\", \"validation_loss\")\n",
        "          self.plot(self.train_acc_l, self.test_acc_l, \"train_acc\", \"validation_acc\")\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "      #self.plot()\n",
        "      # train_loss = float(loss_sum / batch_index)\n",
        "      # self.plot_train_losses.append(train_loss)\n",
        "      #print(f\"Epoch: {epoch}. Loss: {train_loss}\")\n",
        "      # test_loss = self.test_loss()\n",
        "      self.scheduler.step(test_loss)\n",
        "      # self.plot_test_losses.append(test_loss)\n",
        "\n",
        "    # self.test_loss()\n",
        "  \n",
        "  \n",
        "  \n",
        "  def test_loss(self):\n",
        "    with torch.no_grad():\n",
        "      loss_sum = 0\n",
        "      batch_index = 0\n",
        "      correct_cnt = 0\n",
        "      for X, mask, label in self.dataloader2:\n",
        "        batch_index += 1\n",
        "        X = X.permute([1, 0, 2])\n",
        "        X = X.to(self.device)\n",
        "        label = label.long().to(self.device)\n",
        "        if (len(label)) < self.batch_size:\n",
        "          continue\n",
        "        label = label.reshape(self.batch_size)\n",
        "        output = self.net(X)\n",
        "        loss = self.loss_function(output, label)\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        pred = torch.max(output, axis=1).indices      \n",
        "        eq = torch.eq(label, pred)\n",
        "        # print(\"pred\", pred)     \n",
        "        # print(\"label\", label)\n",
        "        correct_cnt += torch.sum(eq)\n",
        "      \n",
        "\n",
        "    avg_loss = loss_sum / (batch_index * self.batch_size)\n",
        "    # print(\"denom\", batch_index * self.batch_size)\n",
        "    # print(\"num\", correct_cnt * self.batch_size)\n",
        "    acc = float(correct_cnt/ (batch_index * self.batch_size))\n",
        "    #print(\"acc:\", acc)\n",
        "    return avg_loss, acc\n",
        "    \n",
        "  def plot(self, l1, l2, label1, label2):\n",
        "    plt.plot(l1, 'b', label=label1)\n",
        "    plt.plot(l2, 'g', label=label2)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "#T = TrainRaw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuQJx25yvmBd"
      },
      "source": [
        "trained_text_model = TrainRaw()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFRJMRP5G8pj"
      },
      "source": [
        "saved_text_model = {\n",
        "    'train_epochs': 20,\n",
        "    'model': trained_text_model.net.state_dict(),\n",
        "    'optimizer': trained_text_model.optimizer.state_dict(),\n",
        "    'criterion': trained_text_model.loss_function.state_dict(),\n",
        "    'scheduler': trained_text_model.scheduler.state_dict(),\n",
        "    'plot_train_losses': trained_text_model.plot_train_losses,\n",
        "    'plot_test_losses': trained_text_model.plot_test_losses,\n",
        "    'test_acc_l': trained_text_model.test_acc_l,\n",
        "    'train_acc_l':  trained_text_model.train_acc_l\n",
        "}\n",
        "\n",
        "torch.save(saved_text_model, 'gdrive/MyDrive/complete_model{}'.format(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOYxIrn0LZpA",
        "outputId": "7098738b-51df-40e4-a89a-38d795360245"
      },
      "source": [
        "checkpoint = torch.load('gdrive/MyDrive/complete_model{}')\n",
        "trained_text_model = LSTM_Classifer(input_dim=768, hidden_dim=100, output_dim=100, class_num=6).to(device)\n",
        "trained_text_model.load_state_dict(checkpoint['model'])\n",
        "trained_text_model.eval()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LSTM_Classifer(\n",
              "  (lstm): LSTM(768, 100)\n",
              "  (fc1): Linear(in_features=100, out_features=100, bias=True)\n",
              "  (act1): ReLU()\n",
              "  (output_layer): Linear(in_features=100, out_features=6, bias=True)\n",
              "  (dropoutLayer): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywWeAsaF1eVe"
      },
      "source": [
        "# Audio Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYwCQvi-1k6B"
      },
      "source": [
        "# path\n",
        "# waveform\n",
        "# sample_rate\n",
        "# emotion\n",
        "# activation\n",
        "# valence\n",
        "# dominance\n",
        "audio_d = pickle.load(open(\"gdrive/MyDrive/audio.pickle\", \"rb\"), encoding='latin1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEc4z6-f11rY"
      },
      "source": [
        "class Iemocap_audio_dataset(object):\n",
        "\n",
        "  #Vids is either trainVid or testVid\n",
        "  def __init__(self, Vids):\n",
        "      self.audio_data = []\n",
        "      for vid in Vids:\n",
        "        for utt in videoIDs[vid]:\n",
        "          cur_utt = audio_d[vid][utt]\n",
        "\n",
        "          sample = {\n",
        "            'path': cur_utt['path'],\n",
        "            'waveform': torch.tensor(cur_utt['waveform']),\n",
        "            'sample_rate': cur_utt['sample_rate'],\n",
        "            'emotion': cur_utt['emotion'],\n",
        "            'activation': cur_utt['activation'],\n",
        "            'valence': cur_utt['valence'],\n",
        "            'dominance': cur_utt['dominance']\n",
        "          }\n",
        "          self.audio_data.append(sample)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.audio_data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    if torch.is_tensor(idx):\n",
        "      idx = idx.tolist()\n",
        "    return self.audio_data[idx]\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOf8qD_dG6w9"
      },
      "source": [
        "# train_audio =  Iemocap_audio_dataset(trainVid)\n",
        "# test_audio =  Iemocap_audio_dataset(testVid)\n",
        "# print(len(audio_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XhE3sk_SEg0",
        "outputId": "99ba7534-577d-4bcf-aa4c-527af354a92f"
      },
      "source": [
        "# iemocap_dataset = train_audio\n",
        "# def f():\n",
        "#     cnt = 0\n",
        "#     for i in range(len(iemocap_dataset)):\n",
        "#         sample = iemocap_dataset[i]\n",
        "#         print(type(sample))\n",
        "#         print(i, sample)\n",
        "#         cnt += 1\n",
        "#         if cnt > 1:\n",
        "#             return\n",
        "# f()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'dict'>\n",
            "0 {'path': '/home/yudong/Private/datasets/IEMOCAP/IEMOCAP_full_release/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_F000.wav', 'waveform': tensor([[-0.0050, -0.0050, -0.0038,  ..., -0.0027, -0.0032, -0.0042]]), 'sample_rate': 16000, 'emotion': 2.0, 'activation': 2.5, 'valence': 2.5, 'dominance': 2.5}\n",
            "<class 'dict'>\n",
            "1 {'path': '/home/yudong/Private/datasets/IEMOCAP/IEMOCAP_full_release/IEMOCAP_full_release/Session1/sentences/wav/Ses01F_impro01/Ses01F_impro01_M000.wav', 'waveform': tensor([[-0.0035, -0.0031, -0.0063,  ...,  0.0034,  0.0027,  0.0026]]), 'sample_rate': 16000, 'emotion': 5.0, 'activation': 2.5, 'valence': 2.0, 'dominance': 2.5}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtz3hfM2HqgD"
      },
      "source": [
        "def collage_fn_vgg(batch):\n",
        "        # Clip or pad each utterance audio into 4.020 seconds.\n",
        "        sample_rate = 16000\n",
        "        n_channels = 1\n",
        "        frame_length = int(4.020 * sample_rate)\n",
        "\n",
        "        # Initialize output\n",
        "        waveforms = torch.zeros(0, n_channels, frame_length)\n",
        "        emotions = torch.zeros(0)\n",
        "\n",
        "        for item in batch:\n",
        "            waveform = item['waveform']\n",
        "            original_waveform_length = waveform.shape[1]\n",
        "            padded_waveform = F.pad(waveform, (0, frame_length - original_waveform_length)) if original_waveform_length < frame_length else waveform[:, :frame_length]\n",
        "            waveforms = torch.cat((waveforms, padded_waveform.unsqueeze(0)))\n",
        "            emotions = torch.cat((emotions, torch.tensor([item['emotion']])), 0)\n",
        "\n",
        "        return waveforms, emotions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXpYEjzJIT6O"
      },
      "source": [
        "# Train audio modality"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFU6MRDSLNea"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class VGG_convnet(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(VGG_convnet, self).__init__()\n",
        "\n",
        "        # block 1:        1 x 64240 --> 64 x 200\n",
        "        self.conv1a = nn.Conv1d(1, 64, kernel_size=400, stride=160)\n",
        "        self.conv1b = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
        "        self.pool1 = nn.MaxPool1d(2)\n",
        "\n",
        "        # block 2:          64 x 200 --> 128 x 50\n",
        "        self.conv2a = nn.Conv1d(64, 128, kernel_size=3, padding=1)\n",
        "        self.conv2b = nn.Conv1d(128, 128, kernel_size=3, padding=1)\n",
        "        self.pool2 = nn.MaxPool1d(4)\n",
        "\n",
        "        # block 3:          128 x 50 --> 256 x 12\n",
        "        self.conv3a = nn.Conv1d(128, 256, kernel_size=3, padding=1)\n",
        "        self.conv3b = nn.Conv1d(256, 256, kernel_size=3, padding=1)\n",
        "        self.pool3 = nn.MaxPool1d(4)\n",
        "\n",
        "        # block 4:          256 x 12 --> 512 x 4\n",
        "        self.conv4a = nn.Conv1d(256, 512, kernel_size=3, padding=1)\n",
        "        self.pool4 = nn.MaxPool1d(3)\n",
        "\n",
        "        # linear layers:    512 x 4 --> 2048 --> 4096 --> 4096 --> 9\n",
        "        self.linear1 = nn.Linear(2048, 4096)\n",
        "        self.linear2 = nn.Linear(4096, 4096)\n",
        "        self.linear3 = nn.Linear(4096, 100)\n",
        "        self.linear4 = nn.Linear(100, 6)\n",
        "\n",
        "    def forward(self, x, get_embed=False):\n",
        "\n",
        "        # block 1\n",
        "        x = self.conv1a(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv1b(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        # block 2\n",
        "        x = self.conv2a(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2b(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        # block 3\n",
        "        x = self.conv3a(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv3b(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool3(x)\n",
        "\n",
        "        # block 4:\n",
        "        x = self.conv4a(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.pool4(x)\n",
        "\n",
        "        # linear layers\n",
        "        x = x.view(-1, 2048)\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear3(x)\n",
        "        x = F.relu(x)\n",
        "        if get_embed:\n",
        "          return x\n",
        "        x = self.linear4(x)\n",
        "\n",
        "        return x\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfYSoKx4I2oM"
      },
      "source": [
        "audio_d = pickle.load(open(\"gdrive/MyDrive/audio.pickle\", \"rb\"), encoding='latin1')\n",
        "audio_trainset =  Iemocap_audio_dataset(trainVid)\n",
        "audio_testset =  Iemocap_audio_dataset(testVid)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEoF67JhIauX"
      },
      "source": [
        "import copy\n",
        "from torch.optim import lr_scheduler\n",
        "class Train_audio():\n",
        "  def __init__(self):\n",
        "    self.audio_trainset = audio_trainset\n",
        "    self.audio_testset = audio_testset\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(self.device)\n",
        "    self.batch_size = 35\n",
        "    self.EPOCHS = 10\n",
        "\n",
        "    self.train_loss_l = []\n",
        "    self.test_loss_l = []\n",
        "\n",
        "    self.train_acc_l = []\n",
        "    self.test_acc_l = []\n",
        "\n",
        "\n",
        "    # self.dataloader1 = DataLoader(self.audio_trainset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "    # self.dataloader2 = DataLoader(self.audio_testset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "    datasets = {\n",
        "    'train': self.audio_trainset,\n",
        "    'val': self.audio_testset }\n",
        "\n",
        "    dataset_sizes = { x: len(datasets[x]) for x in ['train', 'val'] }\n",
        "    self.dataloaders = { x: torch.utils.data.DataLoader(datasets[x], batch_size=self.batch_size, shuffle=True, num_workers=4, collate_fn=collage_fn_vgg) for x in ['train', 'val'] }\n",
        "    self.dataset_sizes = { x: len(datasets[x]) for x in ['train', 'val'] }\n",
        "    self.model = VGG_convnet().to(self.device)\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "    self.optimizer_ft = optim.Adam(self.model.parameters(), lr=0.001)\n",
        "    self.exp_lr_scheduler = lr_scheduler.StepLR(self.optimizer_ft, step_size=7, gamma=0.1)\n",
        "    self.model = self.train(self.model, self.criterion, self.optimizer_ft,  self.exp_lr_scheduler, self.EPOCHS)\n",
        "    self.datasets = datasets\n",
        "\n",
        "\n",
        "  \n",
        "  def train(self, model, criterion, optimizer, scheduler, num_epochs):\n",
        "    best_acc = 0\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "       \n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()   # Set model to training mode\n",
        "            else:\n",
        "                model.eval()    # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in self.dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.long().to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / self.dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / self.dataset_sizes[phase]\n",
        "\n",
        "            if phase == 'train':\n",
        "              self.train_loss_l.append(epoch_loss)\n",
        "              self.train_acc_l.append(epoch_acc)\n",
        "            \n",
        "            else:\n",
        "              self.test_loss_l.append(epoch_loss)\n",
        "              self.test_acc_l.append(epoch_acc)\n",
        "\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    \n",
        "    self.plot(self.train_loss_l, self.test_loss_l, \"train_loss\", \"validation_loss\")\n",
        "    self.plot(self.train_acc_l, self.test_acc_l, \"train_acc\", \"validation_acc\")\n",
        "\n",
        "\n",
        "    #time_elapsed = time.time() - since\n",
        "    #print('raining complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "  \n",
        "\n",
        "  def get_embed(self):\n",
        "    dataloaders = { x: torch.utils.data.DataLoader(self.datasets[x], batch_size=35, shuffle=False, num_workers=4, collate_fn=collage_fn_vgg) for x in ['train', 'val'] }\n",
        "    d = dict()\n",
        "    with torch.no_grad():\n",
        "      for phase in ['train', 'val']:\n",
        "        d[phase] = []\n",
        "        for inputs, labels in dataloaders[phase]:\n",
        "          inputs = inputs.to(device)\n",
        "          labels = labels.long().to(device)\n",
        "          outputs = self.model(inputs, get_embed=True)\n",
        "          d[phase].append(outputs)\n",
        "        d[phase] = torch.cat(d[phase], dim=0)\n",
        "    return d\n",
        "\n",
        "\n",
        "  def plot(self, l1, l2, label1, label2):\n",
        "    plt.plot(l1, 'b', label=label1)\n",
        "    plt.plot(l2, 'g', label=label2)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n25Pmyugdy8C"
      },
      "source": [
        "# d = train.get_embed()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAwxgEnyfYjZ",
        "outputId": "48e98307-f834-4862-ad68-749cc287a60f"
      },
      "source": [
        "# tr = d['train']\n",
        "# print(np.shape(tr))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([5810, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOM4AfUnq2rr",
        "outputId": "473bdc06-8171-4380-9186-045358986443"
      },
      "source": [
        "# print(np.shape(tr[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0pKDluBkPBNZ",
        "outputId": "3106243d-4b49-4651-d670-8b16d21202ac"
      },
      "source": [
        "trained_audio_model = Train_audio()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n",
            "Epoch 0/9\n",
            "train Loss: 1.7430 Acc: 0.2361\n",
            "val Loss: 1.7489 Acc: 0.2292\n",
            "Epoch 1/9\n",
            "train Loss: 1.6411 Acc: 0.2928\n",
            "val Loss: 1.6036 Acc: 0.2951\n",
            "Epoch 2/9\n",
            "train Loss: 1.5412 Acc: 0.3552\n",
            "val Loss: 1.5675 Acc: 0.3580\n",
            "Epoch 3/9\n",
            "train Loss: 1.5187 Acc: 0.3747\n",
            "val Loss: 1.5725 Acc: 0.3420\n",
            "Epoch 4/9\n",
            "train Loss: 1.4934 Acc: 0.3842\n",
            "val Loss: 1.5743 Acc: 0.3623\n",
            "Epoch 5/9\n",
            "train Loss: 1.4863 Acc: 0.3960\n",
            "val Loss: 1.5752 Acc: 0.3426\n",
            "Epoch 6/9\n",
            "train Loss: 1.4674 Acc: 0.4040\n",
            "val Loss: 1.6360 Acc: 0.3715\n",
            "Epoch 7/9\n",
            "train Loss: 1.4181 Acc: 0.4182\n",
            "val Loss: 1.5549 Acc: 0.3715\n",
            "Epoch 8/9\n",
            "train Loss: 1.3961 Acc: 0.4301\n",
            "val Loss: 1.5575 Acc: 0.3580\n",
            "Epoch 9/9\n",
            "train Loss: 1.3842 Acc: 0.4349\n",
            "val Loss: 1.5618 Acc: 0.3604\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Best val Acc: 0.371534\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inBeJ8tyz6mM"
      },
      "source": [
        "#LSTM Utterance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsj55k9Gz6Di"
      },
      "source": [
        "class LSTM_Utter(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, num_classes, dropout = 0.5):\n",
        "    super(LSTM_Utter, self ).__init__() \n",
        "    self.num_classes = num_classes\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.dropout_rate = dropout\n",
        "    self.lstm = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, num_layers = 2, bidirectional = True, dropout=dropout)\n",
        "    self.actattn = nn.ReLU()\n",
        "    self.attn_dim = 2 * hidden_dim\n",
        "    self.attentionfc = nn.Linear(self.attn_dim, self.attn_dim)\n",
        "\n",
        "    # enter linear classifier after exiting from attention\n",
        "    self.fc1 = nn.Linear(self.attn_dim, 100)\n",
        "    self.act1 = nn.ReLU()\n",
        "    self.output_layer = nn.Linear(100, self.num_classes)\n",
        "    self.dropout_layer = nn.Dropout(dropout)\n",
        "\n",
        "  def attention(self, x_encode, x_lstmout, mask):\n",
        "    # assert(output)\n",
        "    # x_encode => (batch, hidden)\n",
        "    # M => (seqlen, batch, output)\n",
        "    # mask => (batch, seqlen)\n",
        "    # print(\"x_encode\",np.shape(x_encode))\n",
        "    # print(\"x_lstmout\", np.shape(x_lstmout))\n",
        "    # print(\"mask\",np.shape(mask))\n",
        "    mask_seq = mask.unsqueeze(2)\n",
        "    mask_seq = mask_seq.repeat(1, 1, self.attn_dim) #mask_seq => (batch, seqlen, output)\n",
        "    transposed_seq = x_lstmout.transpose(0, 1) #transposed_seq => (batch, seqlen, output)\n",
        "    masked_output = transposed_seq * mask_seq #masked_output => (batch, seqlen, output)\n",
        "\n",
        "    x_hidden = self.attentionfc(x_encode)\n",
        "    x_hidden = torch.unsqueeze(x_hidden, 1)  #x_hidden => (batch, 1, output) \n",
        "\n",
        "    attn = torch.bmm(x_hidden, masked_output.transpose(1, 2)) #attn => (batch, 1, seqlen)\n",
        "    masked_attn = torch.tanh(attn * torch.unsqueeze(mask, 1)) #attn => (batch, 1, seqlen)\n",
        "    normalized_attn = F.softmax(masked_attn, dim=2) #normalized_attn => (batch, 1, seqlen)\n",
        "\n",
        "    attn_sum = torch.sum(normalized_attn * torch.unsqueeze(mask, 1), dim=2, keepdim=True) # batch, 1, 1\n",
        "    normalized_attn = normalized_attn / attn_sum  # batch, 1, seqlen ; normalized\n",
        "    attn_pool = torch.bmm(normalized_attn, transposed_seq) # batch, 1, output\n",
        "    return attn_pool[:,0,:]\n",
        "\n",
        "  def forward(self, x, mask):\n",
        "    x_lstmout, (hidden_res,c) = self.lstm(x)\n",
        "    attention_info = []\n",
        "\n",
        "    for x_encode in x_lstmout:\n",
        "      attn_pool = self.attention(x_encode, x_lstmout, mask)\n",
        "      attn_pool = torch.unsqueeze(attn_pool, 0)\n",
        "      attention_info.append(attn_pool)\n",
        "\n",
        "    attn_res  = torch.cat(attention_info,dim=0) # => seqlen, batch, output\n",
        "    res = self.fc1(attn_res)\n",
        "    res = self.act1(res)\n",
        "    res = self.dropout_layer(res)\n",
        "    out = self.output_layer(res) # => (seqlen, batch, 6) \n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yAbT6BOrro_n"
      },
      "source": [
        "# Utterance: data preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QutAh7_q6njs"
      },
      "source": [
        "\n",
        "*   train_text -> encoder -> train_encoded\n",
        "\n",
        "*   test_text -> encoder -> test_encoded\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNmWvFdioLsk"
      },
      "source": [
        "embed_audio_train = trained_audio_model.get_embed()['train'].detach().to('cpu')\n",
        "embed_audio_test = trained_audio_model.get_embed()['val'].detach().to('cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gl0WdV-7rt1K"
      },
      "source": [
        "def utterances_padding(x, max_utterance_len):\n",
        "  assert(len(x.shape) == 2)\n",
        "  encoded_dim = x.shape[1]\n",
        "  seqlen = x.shape[0]\n",
        "  padding = torch.zeros(max_utterance_len - seqlen, encoded_dim).float()\n",
        "  padded_res = np.concatenate((x, padding))\n",
        "  seqlen = x.shape[0]\n",
        "  mask = torch.zeros(max_utterance_len, encoded_dim).float()\n",
        "  mask[0:seqlen, :] = 1.0\n",
        "  return padded_res, mask\n",
        "\n",
        "def label_padding(label, max_utterance_len):\n",
        "  seqlen = label.shape[0]\n",
        "  padding = torch.zeros(max_utterance_len - seqlen).float()\n",
        "  padded_res = np.concatenate((label, padding))\n",
        "\n",
        "  mask = torch.zeros(max_utterance_len).float()\n",
        "  mask[0:seqlen] = 1.0\n",
        "  return padded_res, mask\n",
        "\n",
        "\n",
        "(utterance_count, max_sentence_len, 768) => encode => (utterances_count, hidden_size)\n",
        "def get_iemocap_encoded(model, sentence_max_len):\n",
        "  #model should be pretrained LSTM_Classifier type \n",
        "  #sentence_max_len <=> model\n",
        "  train_video = []\n",
        "  train_video_label = []\n",
        "  test_video = []\n",
        "  test_video_label = []\n",
        "\n",
        "  # embed_audio_train = train.get_embed()['train'].detach().to('cpu')\n",
        "  # embed_audio_test = train.get_embed()['val'].detach().to('cpu')\n",
        "  # cnt = 0\n",
        "  idx = -1\n",
        "  for vid in trainVid:\n",
        "    # cnt += 1\n",
        "    # if (cnt > 2):\n",
        "    #   break\n",
        "    vid_sentences = []\n",
        "    vid_audio = []\n",
        "    vid_labels = videoLabels[vid]\n",
        "    vid_labels = torch.Tensor(vid_labels).float()\n",
        "    vid_labels.requires_grad = False\n",
        "    for i in range(len(videoSentence[vid])):\n",
        "      idx += 1\n",
        "\n",
        "      cur_sentence = videoSentence[vid][i]\n",
        "      sentence_tokens = generate_tokens(cur_sentence)\n",
        "      embeded, mask = bert_embed(sentence_tokens, sentence_max_len)\n",
        "      mask = torch.Tensor(mask).float()\n",
        "\n",
        "      embeded_no_grad = embeded.detach() #=> (max_sentence_len, 768) \n",
        "      mask.requires_grad = False\n",
        "\n",
        "      vid_sentences.append(torch.tensor(np.expand_dims(embeded_no_grad, axis=0)).to(device))\n",
        "      vid_audio.append(embed_audio_train[idx])\n",
        "    \n",
        "    vid_sentences = torch.cat(vid_sentences, dim=0) #(utterance_count, max_sentence_len, 768)\n",
        "    vid_sentences = vid_sentences.permute([1, 0, 2]) #(max_sentence_len, utterance_count, 768)\n",
        "    encoded_video = model.encode_feature(vid_sentences).detach().to('cpu')  #(utterances_count, hidden_size=100)\n",
        "\n",
        "\n",
        "    # vid_audio = torch.tensor(vid_audio)\n",
        "    vid_audio = torch.stack(vid_audio) #(utterances_count, hidden_size=100)\n",
        "   \n",
        "\n",
        "\n",
        "    text_audio = torch.cat((encoded_video, vid_audio), 1) #(utterances_count, 2*hidden_size=200)\n",
        "\n",
        "    train_video.append(text_audio)\n",
        "    train_video_label.append(vid_labels)\n",
        "\n",
        "\n",
        "\n",
        "  idx = -1\n",
        "  for vid in testVid:\n",
        "    vid_sentences = []\n",
        "    vid_audio = []\n",
        "    vid_labels = videoLabels[vid]\n",
        "    vid_labels = torch.Tensor(vid_labels).float()\n",
        "    vid_labels.requires_grad = False\n",
        "    for i in range(len(videoSentence[vid])):\n",
        "      idx += 1\n",
        "      cur_sentence = videoSentence[vid][i]\n",
        "      sentence_tokens = generate_tokens(cur_sentence)\n",
        "      embeded, mask = bert_embed(sentence_tokens, sentence_max_len)\n",
        "      mask = torch.Tensor(mask).float()\n",
        "\n",
        "      embeded_no_grad = embeded.detach()\n",
        "      mask.requires_grad = False\n",
        "      vid_sentences.append(torch.tensor(np.expand_dims(embeded_no_grad, axis=0)).to(device))\n",
        "      vid_audio.append(embed_audio_test[idx])\n",
        "\n",
        "    vid_sentences = torch.cat(vid_sentences, dim=0) #(utterance_count, max_sentence_len, 768)\n",
        "    vid_sentences = vid_sentences.permute([1, 0, 2])\n",
        "    encoded_video = model.encode_feature(vid_sentences).detach().to('cpu')  #(utterances_count, hidden_size)\n",
        "    \n",
        "    vid_audio = torch.stack(vid_audio) #(utterances_count, hidden_size=100)\n",
        "    text_audio = torch.cat((encoded_video, vid_audio), 1) #(utterances_count, 2*hidden_size=200)\n",
        "    \n",
        "    test_video.append(text_audio)\n",
        "    test_video_label.append(vid_labels)\n",
        "  \n",
        "  return train_video, train_video_label, test_video, test_video_label\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IPpwXtpgaCk"
      },
      "source": [
        "class Video_Dataset(Dataset):\n",
        "  def __init__(self, videos, video_labels, max_utterance):\n",
        "    self.data = videos\n",
        "    self.video_labels = video_labels\n",
        "    self.max_utterance = max_utterance\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    video_utterances = self.data[idx] #=> (utterances, hidden)\n",
        "    label = self.video_labels[idx]\n",
        "    padded_utterances, utt_mask = utterances_padding(video_utterances, self.max_utterance)\n",
        "    #padded_utterances => (max_utterances, hidden of encoder)\n",
        "    #mask => (max_utterances) 0 or 1\n",
        "    padded_utterances = torch.Tensor(padded_utterances).float()\n",
        "    padded_utterances.requires_grad = False\n",
        "    utt_mask.requires_grad = False\n",
        "    padded_label, label_mask = torch.Tensor(label_padding(label, self.max_utterance)).float()\n",
        "    padded_label.requires_grad = False\n",
        "    label_mask.requires_grad = False\n",
        "    #padded_utterance => (max_utterances, hidden)\n",
        "    #label => (max_utterances)\n",
        "    #utt_mask => (max_utterances, hidden)\n",
        "    #label_mask => (max_utterances)\n",
        "    return padded_utterances, padded_label, utt_mask, label_mask\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st7mSyqUj_DY"
      },
      "source": [
        "# LSTM_Utterance  Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osDCDrPqfZAy"
      },
      "source": [
        "train_video, train_video_label, test_video, test_video_label = get_iemocap_encoded(trained_text_model, 49)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VOLQdQ2Jj2ru"
      },
      "source": [
        "import itertools\n",
        "\n",
        "class Train_LSTM_Utter():\n",
        "  def __init__(self, model, sentence_max_len):\n",
        "    self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(self.device)\n",
        "    self.max_len = 10\n",
        "    self.batch_size = 4\n",
        "    self.plot_every = 8 # plot the avg loss of 8 batches\n",
        "\n",
        "    #get iemocap text data\n",
        "    #self.train_video, self.train_video_label, self.test_video, self.test_video_label = get_iemocap_encoded(model, sentence_max_len)\n",
        "    self.train_video, self.train_video_label, self.test_video, self.test_video_label = train_video, train_video_label, test_video, test_video_label\n",
        "    self.train_dataset = Video_Dataset(self.train_video, self.train_video_label, 110)\n",
        "    self.test_dataset = Video_Dataset(self.test_video, self.test_video_label, 110)\n",
        "\n",
        "    self.dataloader1 = DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "    self.dataloader2 = DataLoader(self.test_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "    \n",
        "    self.net = LSTM_Utter(input_dim=model.output_dim, hidden_dim = 100, num_classes = 6).to(self.device)\n",
        "\n",
        "    self.plot_train_losses = []\n",
        "    self.plot_test_losses = []\n",
        "    self.test_acc_l = []\n",
        "    self.train_acc_l = []\n",
        "\n",
        "    self.acc_l = []\n",
        "    self.train()\n",
        "\n",
        "  def train(self):\n",
        "    lrate = 0.00005\n",
        "    print(lrate)\n",
        "    EPOCHS = 150\n",
        "    self.optimizer = optim.Adam(self.net.parameters(), lr=lrate)\n",
        "    self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, factor=0.75, patience=2, verbose=True)\n",
        "    #self.exp_lr_scheduler = lr_scheduler.StepLR(self.optimizer, step_size=7, gamma=0.1)\n",
        "    self.loss_function = nn.CrossEntropyLoss()\n",
        "    for epoch in range(EPOCHS):\n",
        "      batch_index = 0\n",
        "      loss_sum = 0\n",
        "      utt_count = 0\n",
        "      \n",
        "      for  X, label, utt_mask, label_mask in self.dataloader1:\n",
        "        # X => (batch, max_utterances, encoded_dim)\n",
        "        # label => (batch, max_utterances) each entry between 1 and 6\n",
        "        # utt_mask => (batch, max_utterances, encoded_dim) each entry either 0 or 1\n",
        "        # label_mask => (batch, max_utterances) each entry either 0 or 1\n",
        "        self.optimizer.zero_grad()\n",
        "        batch_index += 1\n",
        "        #print(\"x:\",np.shape(X), \"label\",np.shape(label),\"utt_mask\",np.shape(utt_mask), \"label_mask\",np.shape(label_mask)  )\n",
        "        X = X.permute([1, 0, 2])\n",
        "        #X =>(max_utterances, batch, hidden)\n",
        "        \n",
        "        X = X.to(self.device)\n",
        "        utt_mask = utt_mask.to(self.device)\n",
        "        label_mask = (label_mask.long()).to(self.device)\n",
        "        label = (label.long().to(self.device)) * label_mask\n",
        "\n",
        "        output = self.net(X, label_mask) \n",
        "        #output  => (max_utterances, batch, 6)\n",
        "\n",
        "        output = (output.permute([1, 0, 2])) #[2, 110, 6])\n",
        "        #print(\"output shape:\", np.shape(output)) \n",
        "        \n",
        "        mask = label_mask.unsqueeze(-1).repeat(1,1,6) #([2, 110, 6])\n",
        "        #print(np.shape(mask))\n",
        "        output *= mask \n",
        "        \n",
        "        #output: (batch, 110, 6) => (batch * 110, 6)\n",
        "        #label: batch, 110 => (batch * 110)\n",
        "        output = output.reshape(-1, 6) \n",
        "        label = label.reshape(-1)    \n",
        "        # print(\"after reshaping:\")\n",
        "        # print(np.shape(mask))\n",
        "        # print(np.shape(label))\n",
        "        \n",
        "        loss = self.loss_function(output, label)     \n",
        "        loss_sum += loss.item()\n",
        "\n",
        "\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        #self.exp_lr_scheduler.step()\n",
        "      \n",
        "      avg_loss = float(loss_sum / batch_index)\n",
        "      self.plot_train_losses.append(avg_loss)\n",
        "      print(\"----------------------\")\n",
        "      print(f\"Epoch: {epoch}. Loss: {loss}\")\n",
        "      test_loss = self.test_loss(True)\n",
        "      self.scheduler.step(test_loss)\n",
        "      self.plot_test_losses.append(test_loss)\n",
        "      self.train_acc()\n",
        "      #self.plot()\n",
        "  \n",
        "  def train_acc(self):\n",
        "    with torch.no_grad():\n",
        "      num_utt = 0\n",
        "      num_correct = 0\n",
        "      batch_index = 0\n",
        "      self.preds = []\n",
        "      self.labels = []\n",
        "      for X, label, utt_mask, label_mask in self.dataloader1:\n",
        "        batch_index += 1\n",
        "        X = X.permute([1, 0, 2])\n",
        "\n",
        "        X = X.to(self.device)\n",
        "        utt_mask = utt_mask.to(self.device)\n",
        "        label_mask = (label_mask.long()).to(self.device)\n",
        "        label = (label.long().to(self.device)) * label_mask\n",
        "        if (len(label)) < self.batch_size:\n",
        "          batch_index -= 1\n",
        "          continue\n",
        "\n",
        "        output = self.net(X, label_mask) \n",
        "        output = (output.permute([1, 0, 2])) #[2, 110, 6])\n",
        "        mask = label_mask.unsqueeze(-1).repeat(1,1,6) #([2, 110, 6])\n",
        "        output *= mask \n",
        "\n",
        "        output = output.reshape(-1, 6) \n",
        "        label = label.reshape(-1)    \n",
        "        \n",
        "        loss = self.loss_function(output, label)     \n",
        "\n",
        "        pred = torch.max(output, axis=1).indices\n",
        "        mask = label_mask.reshape(-1)\n",
        "\n",
        "        \n",
        "        eq = torch.eq(label, pred)\n",
        "        num_utt += torch.sum(mask)\n",
        "        num_correct += torch.sum(mask * eq)\n",
        "      acc = float(num_correct / num_utt)\n",
        "\n",
        "    self.train_acc_l.append(acc)\n",
        "  \n",
        "  def test_loss(self, plot_conf=False):\n",
        "    with torch.no_grad():\n",
        "      test_loss_sum = 0\n",
        "      batch_index = 0\n",
        "      self.preds = []\n",
        "      self.labels = []\n",
        "      num_utt = 0\n",
        "      num_correct = 0\n",
        "      for X, label, utt_mask, label_mask in self.dataloader2:\n",
        "        batch_index += 1\n",
        "        X = X.permute([1, 0, 2])\n",
        "\n",
        "        X = X.to(self.device)\n",
        "        utt_mask = utt_mask.to(self.device)\n",
        "        label_mask = (label_mask.long()).to(self.device)\n",
        "        label = (label.long().to(self.device)) * label_mask\n",
        "        if (len(label)) < self.batch_size:\n",
        "          batch_index -= 1\n",
        "          continue\n",
        "\n",
        "        output = self.net(X, label_mask) \n",
        "        output = (output.permute([1, 0, 2])) #[2, 110, 6])\n",
        "        mask = label_mask.unsqueeze(-1).repeat(1,1,6) #([2, 110, 6])\n",
        "        output *= mask \n",
        "\n",
        "        output = output.reshape(-1, 6) \n",
        "        label = label.reshape(-1)    \n",
        "        \n",
        "        loss = self.loss_function(output, label)     \n",
        "        test_loss_sum += loss.item()\n",
        "\n",
        "        pred = torch.max(output, axis=1).indices\n",
        "        mask = label_mask.reshape(-1)\n",
        "\n",
        "        if plot_conf:\n",
        "          for i in range(len(mask)):\n",
        "            if mask[i] == 0:\n",
        "              continue\n",
        "            self.preds.append(pred[i].item())\n",
        "            self.labels.append(label[i].item())\n",
        "        \n",
        "        eq = torch.eq(label, pred)\n",
        "        num_utt += torch.sum(mask)\n",
        "        num_correct += torch.sum(mask * eq)\n",
        "      acc = float(num_correct / num_utt)\n",
        "    print(\"acc:\", acc)\n",
        "    #print(\"test loss:\", loss_sum, \"num_utt\", num_utt)\n",
        "    self.test_acc_l.append(acc)\n",
        "    return test_loss_sum / batch_index\n",
        "    \n",
        "  def plot(self):\n",
        "    plt.plot(self.plot_train_losses, 'b', label=\"train_loss\")\n",
        "    plt.plot(self.plot_test_losses, 'g', label=\"validation_loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    plt.clf()\n",
        "  \n",
        "\n",
        "  def confusion_matrix(self):\n",
        "    #print(self.labels)\n",
        "    #print(\"------------\")\n",
        "    #print(self.preds)\n",
        "    stacked = torch.stack((torch.tensor(self.labels), torch.tensor(self.preds)),dim=1)\n",
        "    cmt = torch.zeros(6,6, dtype=torch.int64)\n",
        "    for p in stacked:\n",
        "      tl, pl = p.tolist()\n",
        "      cmt[tl, pl] = cmt[tl, pl] + 1\n",
        "    self.plot_confusion_matrix(cmt, range(6))\n",
        "\n",
        "  def plot_confusion_matrix(self, cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = np.array(cm)\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "     "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEmHxb0dHi6V"
      },
      "source": [
        "LSTM = Train_LSTM_Utter(trained_text_model, 49)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWeR15Rqqygj"
      },
      "source": [
        "plt.plot(LSTM.train_acc_l, 'b', label=\"train acc\")\n",
        "plt.plot(LSTM.test_acc_l, 'g', label=\"validation acc\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2VrFuEJhIPd"
      },
      "source": [
        "plt.plot(LSTM.train_acc_l, 'b', label=\"train acc\")\n",
        "plt.plot(LSTM.test_acc_l, 'g', label=\"validation acc\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "th5VKcgNj4TS"
      },
      "source": [
        "# Utterance Preprocessing (Extra)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQN_8hBGrwyY",
        "outputId": "1dfe2b28-2594-4dc4-db91-af63d252adae"
      },
      "source": [
        "############### debug ####################\n",
        "# max_utterance_len = 110\n",
        "x = np.zeros((2,150))\n",
        "pdd, mask = utterance_padding(x, 110)\n",
        "print(np.shape(pdd))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(110, 150)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQ0Gnp_IsfUb"
      },
      "source": [
        "def padd():\n",
        "    videoIDs, videoSpeakers, videoLabels, videoText, videoAudio, videoVisual, videoSentence, trainVid, testVid = pickle.load(open(\"gdrive/MyDrive/IEMOCAP_features_raw.pkl\", \"rb\"), encoding='latin1')\n",
        "    max_utterance_len = 110\n",
        "    video_train_text = [] # (number of videos, max_utterance_len, hiddensize)\n",
        "    video_train_label = []\n",
        "    video_test_text = []\n",
        "    video_test_label = []\n",
        "\n",
        "    index = 0\n",
        "\n",
        "    train = train_encoded\n",
        "    test = test_encoded\n",
        "    for vid in trainVid:\n",
        "        num_utter = len(videoSentence[vid])\n",
        "        \n",
        "        utterances = np.array(train[index: index + num_utter])\n",
        "\n",
        "        for i in range(num_utter):\n",
        "            utterances[i] = np.array(utterances[i].detach())\n",
        "            print(np.shape(utterances[i]))\n",
        "        #print(np.shape(train[0]))\n",
        "        utterances = utterances.reshape(num_utter, -1)\n",
        "        print(utterances.shape)\n",
        "        utterances = utterance_padding(utterances, max_utterance_len)\n",
        "        return\n",
        "\n",
        "        labels = videoLabels[vid]\n",
        "        #print(labels)\n",
        "        padd = [i for i in range(max_utterance_len - num_utter)]\n",
        "        labels += padd\n",
        "\n",
        "        #print(np.shape(utterances))\n",
        "\n",
        "        video_train_text.append(utterances)\n",
        "        video_train_label.append(labels)\n",
        "        index += num_utter\n",
        "    \n",
        "    index = 0\n",
        "    for vid in testVid:\n",
        "        num_utter = len(videoSentence[vid])\n",
        "        \n",
        "        utterances = np.array(test[index: index + num_utter])\n",
        "        utterances = utterances.reshape(num_utter, -1)\n",
        "        utterances = utterance_padding(utterances, max_utterance_len)\n",
        "\n",
        "        labels = videoLabels[vid]\n",
        "        padd = [i for i in range(max_utterance_len - num_utter)]\n",
        "        labels += padd\n",
        "\n",
        "        video_test_text.append(utterances)\n",
        "        video_test_label.append(labels)\n",
        "    \n",
        "    return video_train_text, video_train_label, video_test_text, video_test_label\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBVwNijB_5Wl"
      },
      "source": [
        "# From Poria's official code https://github.com/soujanyaporia/multimodal-sentiment-analysis/blob/master/dataset/iemocap/raw/IEMOCAP_features_raw.pkl.zip\n",
        "videoIDs, videoSpeakers, videoLabels, videoText, videoAudio, videoVisual, videoSentence, trainVid, testVid = pickle.load(open(\"gdrive/MyDrive/IEMOCAP_features_raw.pkl\", \"rb\"), encoding='latin1')\n",
        "\n",
        "#train_size: 120 videos.  test_size: 31 videos\n",
        "\n",
        "\n",
        "def f():\n",
        "    train_text = []\n",
        "    train_seq_len = []\n",
        "    train_label = []\n",
        "\n",
        "    test_text = []\n",
        "    test_seq_len = []\n",
        "    test_label = []\n",
        "\n",
        "    for vid in trainVid:\n",
        "        print(len(videoSentence[vid]), len(videoLabels[vid]), len(videoText[vid]))\n",
        "        for i in range(len(videoSentence[vid])):\n",
        "          cur_sentence = videoSentence[vid][i]\n",
        "          cur_label = videoLabels[vid][i]\n",
        "          train_text.append(cur_sentence)\n",
        "          train_label.append(cur_label)\n",
        "    \n",
        "    for vid in testVid:\n",
        "        for i in range(len(videoSentence[vid])):\n",
        "          cur_sentence = videoSentence[vid][i]\n",
        "          cur_label = videoLabels[vid][i]\n",
        "          test_text.append(cur_sentence)\n",
        "          test_label.append(cur_label)\n",
        "f()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU5Irc-4B6Sz"
      },
      "source": [
        "# Utterance: Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZHlgfhWB-uY"
      },
      "source": [
        "class Utterance_dataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "      \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        data = torch.Tensor(self.data[idx])\n",
        "        label = torch.Tensor(self.labels[idx])\n",
        "\n",
        "        return (data, label)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1vb9Q4kLEfc"
      },
      "source": [
        "ut_train_loader = Utterance_dataset(train_encoded, train_label)\n",
        "ut_test_loader = Utterance_dataset(test_encoded, test_label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2KQxarN1ffe"
      },
      "source": [
        "# Encoder / Decoder: prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpv5QZYj1i-X"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mGUgozf61nDl"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self):\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4SaGCTp1pJu"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# From Poria's official code https://github.com/soujanyaporia/multimodal-sentiment-analysis/blob/master/dataset/iemocap/raw/IEMOCAP_features_raw.pkl.zip\n",
        "videoIDs, videoSpeakers, videoLabels, videoText, videoAudio, videoVisual, videoSentence, trainVid, testVid = pickle.load(open(\"gdrive/MyDrive/IEMOCAP_features_raw.pkl\", \"rb\"), encoding='latin1')\n",
        "\n",
        "#train_size: 120 videos.  test_size: 31 videos\n",
        "\n",
        "\n",
        "def get_iemocap_raw():\n",
        "    train_text = []\n",
        "    train_seq_len = []\n",
        "    train_label = []\n",
        "\n",
        "    test_text = []\n",
        "    test_seq_len = []\n",
        "    test_label = []\n",
        "\n",
        "    for vid in trainVid:\n",
        "        for i in range(len(videoSentence[vid])):\n",
        "          cur_sentence = videoSentence[vid][i]\n",
        "          cur_label = videoLabels[vid][i]\n",
        "          train_text.append(cur_sentence)\n",
        "          train_label.append(cur_label)\n",
        "    \n",
        "    for vid in testVid:\n",
        "        for i in range(len(videoSentence[vid])):\n",
        "          cur_sentence = videoSentence[vid][i]\n",
        "          cur_label = videoLabels[vid][i]\n",
        "          test_text.append(cur_sentence)\n",
        "          test_label.append(cur_label)\n",
        "    \n",
        "    return train_text, train_label, test_text, test_label "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOw3L7cL1sYJ"
      },
      "source": [
        "# Example:\n",
        "# Hi, can I help you?\n",
        "# hi can i help you ?\n",
        "def normalizeString(s):\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "# load data into the dictionary\n",
        "def prepareData():\n",
        "  train_text, train_label, test_text, test_label = get_iemocap_raw()\n",
        "  lang = Lang()\n",
        "  cur = 0\n",
        "\n",
        "  MAX_LENGTH = 135\n",
        "\n",
        "  for s in train_text: # example of s: Hi, can I help you?\n",
        "    s = normalizeString(s)\n",
        "    lang.addSentence(s)\n",
        "  \n",
        "  for s in test_text:\n",
        "    s = normalizeString(s)\n",
        "    lang.addSentence(s)\n",
        "\n",
        "  return lang\n",
        "\n",
        "# prepareData()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UX2ONnV2Tb-"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vk8I4lkJ18FR"
      },
      "source": [
        "# Encoder / Decoder: Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bUKLWqXt2ATI"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "########################################################\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "\n",
        "\n",
        "########################################################\n",
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=135):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY7Upqnt2XuZ"
      },
      "source": [
        "# Encoder / Decoder: train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDpMppie2dI_"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=135):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "  \n",
        "def evaluate(input_tensor, target_tensor, encoder, decoder, criterion, max_length=135):\n",
        "    with torch.no_grad():\n",
        "        input_length = input_tensor.size(0)\n",
        "        target_length = target_tensor.size(0)\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        loss = 0\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        #decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "        if use_teacher_forcing:\n",
        "            # Teacher forcing: Feed the target as the next input\n",
        "            for di in range(target_length):\n",
        "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs)\n",
        "                loss += criterion(decoder_output, target_tensor[di])\n",
        "                decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "        else:\n",
        "            # Without teacher forcing: use its own predictions as the next input\n",
        "            for di in range(target_length):\n",
        "                decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                    decoder_input, decoder_hidden, encoder_outputs)\n",
        "                topv, topi = decoder_output.topk(1)\n",
        "                decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "                loss += criterion(decoder_output, target_tensor[di])\n",
        "                if decoder_input.item() == EOS_token:\n",
        "                    break\n",
        "\n",
        "\n",
        "        return loss.item()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsLDC4wQ2f7U"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# get raw data and lang dict\n",
        "train_text, train_label, test_text, test_label = get_iemocap_raw()\n",
        "\n",
        "def trainIters(lang, encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.001):\n",
        "    # start = time.time()\n",
        "    EPOCHS = 0\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    \n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    train_losses = []\n",
        "    test_losses = []\n",
        "\n",
        "    \n",
        "\n",
        "    print(\"enter\")\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "      for i in range(len(train_text)):\n",
        "          \n",
        "          s = normalizeString(train_text[i])\n",
        "          input_tensor = tensorFromSentence(lang, s)\n",
        "          target_tensor = input_tensor # want to reconstruct the input from decoder\n",
        "\n",
        "          loss = train(input_tensor, target_tensor, encoder,\n",
        "                      decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "          print_loss_total += loss\n",
        "  \n",
        "\n",
        "          if i > 0 and i % print_every == 0:\n",
        "              print_loss_avg = print_loss_total / print_every\n",
        "              print_loss_total = 0\n",
        "              train_losses.append(print_loss_avg)\n",
        "\n",
        "              # run model on test data\n",
        "              test_loss = 0\n",
        "              for j in range(len(test_text)):\n",
        "                  s = normalizeString(test_text[j])\n",
        "                  input_tensor = tensorFromSentence(lang, s)\n",
        "                  target_tensor = input_tensor\n",
        "                  test_loss += evaluate(input_tensor, target_tensor, encoder,\n",
        "                      decoder, criterion) \n",
        "              test_losses.append(test_loss / len(test_text))\n",
        "\n",
        "      plot(train_losses, test_losses)\n",
        "    \n",
        "    \n",
        "    train_encoded = []\n",
        "    test_encoded = []\n",
        "    max_length=135\n",
        "    for i in range(len(train_text)):\n",
        "      s = normalizeString(train_text[i])\n",
        "      input_tensor = tensorFromSentence(lang, s)\n",
        "      encoder_hidden = encoder.initHidden()\n",
        "      input_length = input_tensor.size(0)\n",
        "      encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "      \n",
        "      for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "      train_encoded.append(encoder_outputs)\n",
        "    \n",
        "    for i in range(len(test_text)):\n",
        "      s = normalizeString(test_text[i])  \n",
        "      input_tensor = tensorFromSentence(lang, s)\n",
        "      encoder_hidden = encoder.initHidden()\n",
        "      input_length = input_tensor.size(0)\n",
        "      encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "      \n",
        "      for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "        input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "      test_encoded.append(encoder_outputs)\n",
        " \n",
        "    print(2)\n",
        "    return train_encoded, test_encoded\n",
        " \n",
        "def plot(train_losses, test_losses):\n",
        "    plt.plot(train_losses, 'b', label=\"train_loss\")\n",
        "    plt.plot(test_losses, 'g', label=\"test_loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3pMYqTW2ljS"
      },
      "source": [
        "**Train commands**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "To2HuL3FE4W0",
        "outputId": "7cff34a3-51cd-4dc0-805f-a2b70117e7f7"
      },
      "source": [
        "hidden_size = 100\n",
        "lang = prepareData()\n",
        "\n",
        "encoder1 = EncoderRNN(lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "train_encoded, test_encoded = trainIters(lang, encoder1, attn_decoder1, 75000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9MAZFHe-GyZ_",
        "outputId": "ecad550c-6641-4167-c27e-5274d84a439f"
      },
      "source": [
        "########### for debug ##############\n",
        "print(np.shape(train_encoded)) # 5810\n",
        "print(np.shape(test_encoded)) # 1623\n",
        "print(np.shape(train_encoded[0])) # (max len hiddeen size)\n",
        "print(np.shape(test_encoded[0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(5810,)\n",
            "(1623,)\n",
            "torch.Size([135, 100])\n",
            "torch.Size([135, 100])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "id": "Lq9hJe542psk",
        "outputId": "0cd58675-bbf6-4f14-a796-00af79d4bb03"
      },
      "source": [
        "########### for debug ##############\n",
        "hidden_size = 128\n",
        "lang = prepareData()\n",
        "\n",
        "encoder1 = EncoderRNN(lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "encoder, decoder = trainIters(lang, encoder1, attn_decoder1, 75000, print_every=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "enter\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ezs1aSjC9hzI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}